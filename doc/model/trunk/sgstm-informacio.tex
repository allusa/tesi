
\chapter{SGSTM i al teoria de la informació}


Teoria de la informació: mesura de la entropia


The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate–distortion theory for lossy compression. 

Information-theoretical foundations for lossy data compression are provided by rate-distortion theory. Much like the use of probability in optimal coding theory, rate-distortion theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment.


\section{Compressió}

La compressió redueix la mida original d'unes dades. 

L'esquema és comprimir->emmagtzemar->descomprimir->visualitzar

compressed data must be decompressed to use -> en els sgstm no? bé si vull veure tota la sèrie temporal sí que he de concatenar el que hi tinc i per tant es pot veure com una descompressió (al marge que hi pot haver una compressió/descompressió en els agregadors usats, per exemple si és un so emmatgatzemar la freqüència i després s'haurà de descomprimir a amplitud al llarg del temps).


* Lossless compression -> per text i fitxers de dades
* Lossy compression -> per multimèdia (àudio, vídeo, imatges)

Ambdós redueixin la mida original, els lossless treuen la redundància que hi pugui haver en les dades (es conserva la informació però s'augmenta l'entropia) i els lossy descarten informació que es considera no essencial (per exemple en una imatge detalls que l'ull humà no pot apreciar) tot i que també poden tractar de cercar una millora representació de les dades (perceptual encoding, per exemple amb la freqüència del so es poden realitzar millor les operacions d'equalització ).

The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (e.g., when using lossy data compression), and the computational resources required to compress and uncompress the data.

Un esquema de compressió típic és tenir un fitxer principal comprimit amb lossless i després altres de petits comprimits amb lossy per a cada aplicació (per exemple imatges web on hi ha la grossa per a ser descarregada però a la galeria d'imatges s'hi col·loquen de petites per no transmetre tanta informació inecessària).



Developing lossy compression techniques as closely matched to human perception as possible is a complex task. Sometimes the ideal is a file that provides exactly the same perception as the original, with as much digital information as possible removed; other times, perceptible loss of quality is considered a valid trade-off for the reduced data.

The advantage of lossy methods over lossless methods is that in some cases a lossy method can produce a much smaller compressed file than any lossless method, while still meeting the requirements of the application.

Lossy methods are most often used for compressing sound, images or videos. This is because these types of data are intended for human interpretation where the mind can easily "fill in the blanks" or see past very minor errors or inconsistencies – ideally lossy compression is transparent (imperceptible), which can be verified via an ABX test. http://en.wikipedia.org/wiki/ABX_test

Flaws caused by lossy compression that are noticeable to the human eye or ear are known as compression artifacts.






The main drawback of lossy compression techniques is that they
rely on specific patterns for providing a good approximation of the given time series.
This is the main reason why lossy compression has been rarely applied to network
monitoring contexts, where the patterns of time series can drastically change due to
anomalous events or to transient networking issues.


% Lossless data compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy data compression, permits reconstruction only of an approximation of the original data, though this usually allows for improved compression rates (and therefore smaller sized files).
% Other approximation approaches are minimal lossy: they try to approximate to the original time series but subsequent compression would not achieve a lossed information. For example storing in the frequency domain is only working with same information in another domain which has a bijective relation to the original. If in this domain there is loss of information then the compression is lossy.

% Lossy compression formats suffer from generation loss: repeatedly compressing and decompressing the file will cause it to progressively lose quality. This is in contrast with lossless data compression, where data will not be lost via the use of such a procedure.

% Information-theoretical foundations for lossy data compression are provided by rate-distortion theory. Much like the use of probability in optimal coding theory, rate-distortion theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment.





\section{Rate–distortion theory}

http://en.wikipedia.org/wiki/Rate-distortion_theory

http://www.data-compression.com/theory.shtml






\section{Criteri RRDtool}

RRDtool expliquen un criteri que és definir les k dels discs inferior a l'amplada en píxels de la pantalla. Aquest és un criteri basat en una consulta per a fer visualització immediatament. Llavors no té sentit tenir més nombre de dades que les que es poden visualitzar. 



\section{Multiresolució sense pèrdua}


Per exemple calcular la mitjana d'una sèrie temporal i calcular la mitjana d'una sèrie temporal multireoslució que tingui un esquema amb agregadors de mitjana de funció, és el mateix? Si hi han dades conegudes per a tot l'interval.

agregador $fmean = \frac{1}{t_f-t_0}\int s(t)$

Per tant $fmean(s)$ hauria de ser igual a $fmean(multiresolution(s))$ perquè $multiresolution(s)= \frac{1}{\delta}\int s[\tau,\tau+\delta]+\frac{1}{\delta}\int s[\tau+\delta,\tau+2\delta]+\dots$ i per tant 

\[
fmean(multiresolution(s)) =  \frac{1}{t_f-t_0}\int\right[\frac{1}{\delta}\int s[\tau,\tau+\delta]+\frac{1}{\delta}\int s[\tau+\delta,\tau+2\delta]+\dots \left]
\]
\[
 =  \frac{1}{t_f-t_0}\frac{\delta}{\delta}\right[\int s[\tau,\tau+\delta]+\int s[\tau+\delta,\tau+2\delta]+\dots \left] =  \frac{1}{t_f-t_0} \int s[t_0,t_f]
\]



\section{Aplicació de la teoria de la informació als SGSTM}

Habitualment en sèries temporals la mesura de qualitat dels algoritmes d'aproximació és la distància/similitud entre la sèrie temporal original i l'aproximada. Això en el cas de SGSTM no té sentit perquè segurament la sèrie temporal comprimida no serà molt similar a l'original.

Això també passa en els algoritmes de compressió amb pèrdua. Els senyals comprimit poden no tenir res a veure amb els originals, però un ésser humà no es capaç de distingir entre els dos: la mesura de similitud és subjectiva. 


En el cas dels SGSTM la mesura de qualitat hauria de ser si donada una consulta dóna  el mateix resultat en la sèrie temporal original que en la comprimida. Per exemple una consulta: hi ha un increment en les dades mesurades.




Semblances dels SGSTM amb la compressió amb pèrdua.

* En el cas de la visualització es pot entendre totalment semblant. Un algoritme de compressió amb pèrdua es pot entendre com una funció f sobre unes dades originals que retorna unes dades noves d'=f(d) i subjectivament es perceben com a semblants o fins i tot no s'hi veu diferència d~d'. Així per exemple en les imatges s'aplica compressió amb pèrdua per a obtenir altres imatges sense detalls que l'ull humà no pot percebre o bé també s'aplica per a reduir les resolucions de la imatge quan els mitjans de reproducció no són capaços de reproduir-les amb tanta qualitat.  Així en les sèries temporals podem fer una similitud si es tracta d'un problema de visualització: no cal visualitzar la sèrie temporal original si aquesta té molta resolució i no som capaços d'apreciar-la. Per exemple si capturem dades cada 5 minuts al llarg d'un any obtenim 43800 punts; si no disposem d'un monitor amb aquest nombre de píxels d'amplada no els podrem percebre.


Diferències dels SGSTM amb la compressió amb pèrdua: 

* No hi ha descompressió. Els algoritmes de compressió amb pèrdua normalment van associats amb les tècniques de compressió/descompressió; és a dir que les dades s'emmagatzemen amb estructures intermitges, que ocupen menys mida, i que cal descomprimir per recuperar-les. En els cas dels SGSTM les dades s'emmagatzemen com a subsèries temporals en els discs i per tant a l'hora de recuperar-les ja són sèries temporals, com a molt potser fa falta concatenar-les per a obtenir tota la sèrie temporal.






%%% Local Variables:
%%% TeX-master: "main"
%%% End:

